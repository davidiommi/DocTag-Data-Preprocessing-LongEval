{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "919d7a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import html\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "73312c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the \"DocTag_LongEval_conf\" directory if it doesn't already exist\n",
    "output_dir = \"DocTag_LongEval_conf_french\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c257ec5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Replace escape characters with their corresponding characters\n",
    "    cleaned_text = html.unescape(text)\n",
    "\n",
    "    # Remove \\u00 sequences\n",
    "    cleaned_text = re.sub(r\"\\\\u00([a-zA-Z0-9]{2})\", lambda m: chr(int(m.group(1), 16)), cleaned_text)\n",
    "\n",
    "    # Replace multiple spaces with a single space\n",
    "    cleaned_text = re.sub(r\"\\s+\", \" \", cleaned_text)\n",
    "\n",
    "    # Remove special characters and numbers\n",
    "    cleaned_text = re.sub(r\"[^a-zA-Z]\", \" \", cleaned_text)\n",
    "\n",
    "    tokens = word_tokenize(cleaned_text)\n",
    "\n",
    "    cleaned_text = \" \".join(tokens)\n",
    "\n",
    "    return cleaned_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8ce686ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_Json_files(input_dir, output_file, doc_ids):\n",
    "    preprocessed_data = []\n",
    "\n",
    "    # Get the list of JSON files in the input directory\n",
    "    input_files = [f for f in os.listdir(input_dir) if f.endswith('.json')]\n",
    "\n",
    "    # Iterate through each file in the input directory with a progress bar\n",
    "    for filename in tqdm(input_files, desc=\"Processing JSON files\"):\n",
    "        # Open the file and load the data into a Python object\n",
    "        with open(os.path.join(input_dir, filename), 'r') as infile:\n",
    "            data = json.load(infile)\n",
    "\n",
    "        # Preprocess and convert the relevant data into the format expected by DocTAG\n",
    "        for item in data:\n",
    "            doc_id = item['id']\n",
    "            if doc_id in doc_ids:\n",
    "                contents = item['contents']\n",
    "                contents = clean_text(contents)\n",
    "                preprocessed_data.append({\n",
    "                    \"document_id\": doc_id,   \n",
    "                    \"text\": contents,\n",
    "                })\n",
    "\n",
    "    # Wrap the preprocessed data in a dictionary with the \"collection\" key\n",
    "    output_data = {\"collection\": preprocessed_data}\n",
    "\n",
    "    # Save the preprocessed data to a new JSON file\n",
    "    with open(output_file, 'w') as outfile:\n",
    "        json.dump(output_data, outfile, indent=2)\n",
    "\n",
    "    print(f\"Preprocessed data saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "09acae34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON files: 100%|█████████████████████████████████████████████████████████| 158/158 [01:09<00:00,  2.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data saved to collection.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 1) Save LongEval/documents collection\n",
    "qrels_file = \"./publish/French/stratified-pool-train-1.txt\" # path to file with queries_id, documents_id \n",
    "qrels_file_description = \"./publish/French/Queries/queries_description.txt\" # path to file with queries_id, queries_description\n",
    "input_dir = \"./publish/French/Documents/Json/\"     # Directory with Json files of the collection\n",
    "output_file = \"collection.json\"     # path to save the collection for Doctag\n",
    "\n",
    "doc_ids = []\n",
    "\n",
    "with open(qrels_file, 'r') as f:\n",
    "    for line in f:\n",
    "        topic_id, doc_id= line.strip().split()\n",
    "        doc_ids.append((doc_id))\n",
    "\n",
    "doc_ids = list(set(doc_ids))  # remove duplicates from documents list\n",
    "doc_ids = sorted(doc_ids, key=str.lower) \n",
    "    \n",
    "preprocess_Json_files(input_dir, output_file, doc_ids)\n",
    "\n",
    "# Move the preprocessed data file to the \"DocTag_LongEval_conf\" directory\n",
    "os.rename(output_file, os.path.join(output_dir, output_file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3f0c964e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('q06221312', 'groupama espace client'), ('q062217180', 'fuseau horaire'), ('q06224677', 'impots gouv simulateur'), ('q06227537', 'gateau a la banane'), ('q06227968', 'location voiture gare de lyon'), ('q062216056', 'achat voiture Ã©lectrique'), ('q062216960', 'emploi store'), ('q062217348', 'hotel blois'), ('q062217472', \"jeux d'argent\"), ('q062220278', 'franÃ§ois sureau')]\n",
      "Topics saved to DocTag_LongEval_conf_french\\topics.json\n"
     ]
    }
   ],
   "source": [
    "# 2) Save LongEval topics\n",
    "\n",
    "topic_ids = []\n",
    "\n",
    "with open(qrels_file, 'r') as f:   # open the files with queries_id, documents_id to extract the queries id\n",
    "    for line in f:\n",
    "        topic_id, doc_id= line.strip().split()\n",
    "        topic_ids .append((topic_id))\n",
    "        \n",
    "topic_ids = sorted(set(topic_ids))  # Sort and remove duplicates\n",
    "\n",
    "\n",
    "filtered_queries = []\n",
    "\n",
    "with open(qrels_file_description, \"r\") as file:  # open the files with queries_id and description\n",
    "    for line in file:\n",
    "        line = line.strip() \n",
    "        if line:  # Skip empty lines\n",
    "            id_, text = line.split(\"\\t\", 1)\n",
    "            if id_ in topic_ids:\n",
    "                filtered_queries.append((id_, text))\n",
    "\n",
    "print(filtered_queries) # Our queries and description filtered from the full list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "topics = []\n",
    "for topic_id, text in filtered_queries:\n",
    "    topics.append({\n",
    "        \"topic_id\": topic_id,\n",
    "        \"title\": text,\n",
    "        \"description\": \"\",\n",
    "        \"narrative\": \"\"\n",
    "    })\n",
    "\n",
    "output_data = {\"topics\": topics}\n",
    "\n",
    "output_file = os.path.join(output_dir, \"topics.json\")\n",
    "with open(output_file, 'w') as outfile:\n",
    "    json.dump(output_data, outfile, indent=2)\n",
    "\n",
    "print(f\"Topics saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f18a8d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output saved to DocTag_LongEval_conf_french\\run.json\n"
     ]
    }
   ],
   "source": [
    "# 3) Save LongEval runs\n",
    "\n",
    "# Read the topic_doc_pairs list from the qrels file\n",
    "doc_topic_pairs = []\n",
    "with open(qrels_file, 'r') as f:\n",
    "    for line in f:\n",
    "        topic_id, doc_id= line.strip().split()\n",
    "        doc_topic_pairs.append((doc_id, topic_id))\n",
    "        \n",
    "runs = []\n",
    "for topic_id in set([pair[1] for pair in doc_topic_pairs]):\n",
    "    documents = [{\"document_id\": pair[0], \"language\": \"french\"} for pair in doc_topic_pairs if pair[1] == topic_id]\n",
    "    runs.append({\"topic_id\": topic_id, \"documents\": documents})\n",
    "\n",
    "output_data = {\"run\": runs}\n",
    "\n",
    "# Save the output to a file in the \"DocTag_LongEval_conf\" directory\n",
    "output_file = os.path.join(output_dir, \"run.json\")\n",
    "with open(output_file, 'w') as outfile:\n",
    "    json.dump(output_data, outfile, indent=2)\n",
    "\n",
    "print(f\"Output saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "28d403ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) LongEval label have been done manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac7ff11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
